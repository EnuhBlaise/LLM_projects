{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyOto0rcP9KAXQotnZIXQ3xj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EnuhBlaise/LLM_projects/blob/main/LLM_Joke_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Using this script to practice using LLMs and ways I can implement them in my research work.\n"
      ],
      "metadata": {
        "id": "ATijF_kRRt-G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJDAtXM4Rfbd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "  def __init__(self):\n",
        "    self.device = 'cuda' if torch.cuda_is_available() else 'cpu'\n",
        "\n",
        "Config.model = 'microsoft/Phi-3-mini-4k-instruct'\n",
        "Config.tokenizer = 'microsoft/Phi-3-mini-4k-instruct'"
      ],
      "metadata": {
        "id": "ozWnBXdIS0C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the model and a tokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(Config.model,\n",
        "                                             device_map='auto',\n",
        "                                             dtype='auto',\n",
        "                                             trust_remote_code=False, #this is true in the book. Caused an error message of tokens not seen. Solution link:https://stackoverflow.com/questions/79769295/attributeerror-dynamiccache-object-has-no-attribute-seen-tokens\n",
        "                                             attn_implementation='eager'\n",
        "                                             )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(Config.model, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "CumSk-ChR_Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "LUFPPy3PTpfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets create a pipeline\n",
        "generator = pipeline('text-generation',\n",
        "                     model=model,\n",
        "                     tokenizer=tokenizer,\n",
        "                     return_full_text=False,\n",
        "                     max_new_tokens=500,\n",
        "                     do_sample=False\n",
        "                     )"
      ],
      "metadata": {
        "id": "3zshtJt5TtXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The prompt is made of a user and their inputor query\n",
        "messages = [{'role':'user', 'content': 'Create a funny joke about chickens.'}]"
      ],
      "metadata": {
        "id": "uHjDzpykUMnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate the output\n",
        "output = generator(messages)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "fxA1N5mWUvWj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}